{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2df08f-d388-4760-b7ba-553e0edd5f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1️⃣ 부서별 서브 부서 확인\n",
    "# ============================================\n",
    "sub_departments_by_dept = jd.groupby('department')['sub_department'].unique()\n",
    "print(sub_departments_by_dept)\n",
    "# 실행 결과 요약: 전체 부서 개수와 각 부서별 서브 부서 리스트가 출력됩니다.\n",
    "\n",
    "# ============================================\n",
    "# 2️⃣ 특정 직무 소개 문구 정제\n",
    "# ============================================\n",
    "import re\n",
    "\n",
    "# 직무 선택\n",
    "original_text = jd[jd['job_title'] == '승용 조립차 생산지원']['organization_intro'].iloc[0]\n",
    "print(\"정제 전 텍스트:\\n\", original_text)\n",
    "\n",
    "# 한글과 공백만 남기기\n",
    "def clean_text(text):\n",
    "    return re.sub('[^가-힣\\s]', '', text)\n",
    "\n",
    "cleaned_text = clean_text(original_text)\n",
    "print(\"\\n정제 후 텍스트:\\n\", cleaned_text)\n",
    "\n",
    "# ============================================\n",
    "# 3️⃣ 한국어 NLP 패키지 설치 및 리소스 다운로드\n",
    "# ============================================\n",
    "!pip install -q konlpy nltk spacy\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "!python -m spacy download ko_core_news_sm\n",
    "\n",
    "# ============================================\n",
    "# 4️⃣ 토큰화 및 비교\n",
    "# ============================================\n",
    "import time\n",
    "from konlpy.tag import Okt, Kkma\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"ko_core_news_sm\")\n",
    "okt = Okt()\n",
    "kkma = Kkma()\n",
    "\n",
    "# 1. Okt 토큰화\n",
    "start_time = time.time()\n",
    "okt_tokens = okt.morphs(cleaned_text)\n",
    "okt_time = time.time() - start_time\n",
    "\n",
    "# 2. Kkma 토큰화\n",
    "start_time = time.time()\n",
    "kkma_tokens = kkma.morphs(cleaned_text)\n",
    "kkma_time = time.time() - start_time\n",
    "\n",
    "# 3. NLTK word_tokenize\n",
    "start_time = time.time()\n",
    "nltk_tokens = nltk.word_tokenize(cleaned_text)\n",
    "nltk_time = time.time() - start_time\n",
    "\n",
    "# 4. spaCy 토큰화\n",
    "start_time = time.time()\n",
    "spacy_tokens = [token.text for token in nlp(cleaned_text)]\n",
    "spacy_time = time.time() - start_time\n",
    "\n",
    "# 출력\n",
    "print(f\"okt: {okt_tokens} ({okt_time:.4f}초)\")\n",
    "print(f\"kkma: {kkma_tokens} ({kkma_time:.4f}초)\")\n",
    "print(f\"word_tokenize: {nltk_tokens} ({nltk_time:.4f}초)\")\n",
    "print(f\"spaCy: {spacy_tokens} ({spacy_time:.4f}초)\")\n",
    "\n",
    "# ============================================\n",
    "# 5️⃣ 품사 태깅 및 명사 추출\n",
    "# ============================================\n",
    "pos_tags = okt.pos(cleaned_text)\n",
    "morphs = okt.morphs(cleaned_text)\n",
    "nouns = [word for word, pos in pos_tags if pos == 'Noun']\n",
    "print(\"품사 태깅:\", pos_tags)\n",
    "print(\"어간 추출:\", morphs)\n",
    "print(\"명사 추출:\", nouns)\n",
    "\n",
    "# ============================================\n",
    "# 6️⃣ 불용어 제거\n",
    "# ============================================\n",
    "stopwords = {\n",
    "    '우리', '자기', '그', '그것', '이', '이것', '저', '나', '너', '등', '대해', '및', \n",
    "    '그런', '또', '하지만', '왜', '때문', '으로', '가', '의', '에', '에서', '로', '와', \n",
    "    '과', '을', '를', '으로', '는', '은', '입니다', '들', '만', '것', '같다', '중',\n",
    "    '사람', '후', '차', '전', '별', '점검', '물량', '환경', '탄력', '물류', '조정'\n",
    "}\n",
    "filtered_nouns = [word for word in nouns if word not in stopwords]\n",
    "print(\"불용어 제거 후 명사:\", filtered_nouns)\n",
    "\n",
    "# 'tokens' 컬럼 추가\n",
    "psatcar_jd.loc[psatcar_jd['job_title']=='승용 조립차 생산지원','tokens'] = [' '.join(filtered_nouns)]\n",
    "print(psatcar_jd[psatcar_jd['job_title']=='승용 조립차 생산지원'][['organization_intro','tokens']].head())\n",
    "\n",
    "# ============================================\n",
    "# 7️⃣ LDA 토픽 모델링\n",
    "# ============================================\n",
    "!pip install gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "# organization_intro 정제 후 명사 추출\n",
    "jd['clean_text'] = jd['organization_intro'].str.replace(r'[^가-힣\\s]', '', regex=True)\n",
    "jd['nouns'] = jd['clean_text'].apply(lambda doc: okt.nouns(doc) if isinstance(doc,str) else [])\n",
    "\n",
    "# 전체 명사 빈도 계산\n",
    "from collections import Counter\n",
    "all_nouns = [n for nouns in jd['nouns'] for n in nouns]\n",
    "noun_freq = Counter(all_nouns)\n",
    "freq_df = pd.DataFrame(noun_freq.items(), columns=['noun','freq']).sort_values('freq',ascending=False).reset_index(drop=True)\n",
    "print(freq_df.head(20))\n",
    "\n",
    "# 상위 10개 단어를 불용어 처리\n",
    "top10_stopwords = set(freq_df.head(10)['noun'])\n",
    "jd['tokens'] = jd['nouns'].apply(lambda noun_list: [w for w in noun_list if w not in top10_stopwords])\n",
    "empty_tokens = jd['tokens'].map(len)==0\n",
    "print(f\"tokens 빈 리스트인 행 개수: {empty_tokens.sum()}\")\n",
    "print(jd[['nouns','tokens']].head())\n",
    "\n",
    "# LDA 모델 학습\n",
    "texts = jd['tokens'].tolist()\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in texts]\n",
    "\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=6, random_state=42, update_every=1, chunksize=100, passes=20, alpha='auto', per_word_topics=True)\n",
    "\n",
    "for tid, terms in lda_model.show_topics(num_topics=6, formatted=False):\n",
    "    print(f\"Topic {tid}: {[w for w,p in terms]}\")\n",
    "\n",
    "# ============================================\n",
    "# 8️⃣ 부서별 LDA 모델링\n",
    "# ============================================\n",
    "grouped = jd.groupby('department')['tokens'].apply(list).to_dict()\n",
    "lda_models = {}\n",
    "for sub_dept, tokens_list in grouped.items():\n",
    "    texts = tokens_list\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    num_topics = len(tokens_list)\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=15, random_state=42)\n",
    "    lda_models[sub_dept] = lda_model\n",
    "\n",
    "    print(f\"부서: {sub_dept}\")\n",
    "    for idx, topic in lda_model.print_topics(num_words=5, num_topics=5):\n",
    "        print(f\"🟦 주제 {idx+1}: {topic}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# pyLDAvis 시각화\n",
    "pyLDAvis.enable_notebook()\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from joblib import parallel_backend\n",
    "with parallel_backend('threading', n_jobs=1):\n",
    "    vis_data = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "vis_data\n",
    "\n",
    "# ============================================\n",
    "# 9️⃣ IT 로그 데이터 처리\n",
    "# ============================================\n",
    "import json\n",
    "with open('IT_logs.json','r',encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "log = pd.DataFrame(data)\n",
    "log['timestamp'] = pd.to_datetime(log['timestamp'])\n",
    "log['hour'] = log['timestamp'].dt.hour\n",
    "log['PC'].value_counts()\n",
    "log['program'] = log['window_title'].str.extract(r'\\[(.*?)\\]')\n",
    "log['file'] = log['window_title'].str.extract(r'-\\s(.+)$')\n",
    "log['merge'] = log['window_title'].str.extract(r'\\[.*\\]\\s*(.*)\\s*-\\s*(.*)')[1]\n",
    "log['day'] = log['timestamp'].dt.date\n",
    "log_grouped = log.groupby(['PC','day'])['merge'].apply(lambda x: list(x)).reset_index()\n",
    "print(log_grouped.head())\n",
    "\n",
    "# ============================================\n",
    "# 10️⃣ Word2Vec 임베딩\n",
    "# ============================================\n",
    "from gensim.models import Word2Vec\n",
    "sentences = log_grouped['merge'].tolist()\n",
    "model = Word2Vec(min_count=1, window=5, vector_size=30, workers=4)\n",
    "model.build_vocab(sentences, progress_per=10000)\n",
    "print(\"Vocabulary Size:\", len(model.wv))\n",
    "print(\"Example words:\", list(model.wv.index_to_key)[:10])\n",
    "\n",
    "# Skip-Gram 모델\n",
    "model = Word2Vec(sentences=sentences, vector_size=30, window=5, min_count=1, sg=1, workers=4, epochs=10)\n",
    "word_vectors = model.wv\n",
    "print(\"Words in vocabulary:\", list(word_vectors.index_to_key)[:10])\n",
    "\n",
    "# ============================================\n",
    "# 11️⃣ t-SNE 시각화\n",
    "# ============================================\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pc_groups = log_grouped.groupby('PC')\n",
    "pc_vectors = []\n",
    "labels = []\n",
    "\n",
    "for pc, group in pc_groups:\n",
    "    for log in group['merge']:\n",
    "        log_vector = np.mean([model.wv[word] for word in log if word in model.wv], axis=0)\n",
    "        if log_vector is not None:\n",
    "            pc_vectors.append(log_vector)\n",
    "            labels.append(pc)\n",
    "\n",
    "pc_vectors_flattened = np.array(pc_vectors)\n",
    "perplexity_value = min(30, len(pc_vectors_flattened)-1)\n",
    "tsne = TSNE(n_components=2, perplexity=perplexity_value, learning_rate=200)\n",
    "transformed_vectors = tsne.fit_transform(pc_vectors_flattened)\n",
    "label_mapping = {label: idx for idx,label in enumerate(sorted(set(labels)))}\n",
    "numeric_labels = [label_mapping[label] for label in labels]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "scatter = plt.scatter(transformed_vectors[:,0], transformed_vectors[:,1], c=numeric_labels, cmap='tab20')\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=label_mapping.keys())\n",
    "plt.title(\"100-day Logs Distribution by PC\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.colorbar(scatter)\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# 12️⃣ RandomForest 기반 PC 분류\n",
    "# ============================================\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(pc_vectors_flattened, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=9,\n",
    "    max_depth=3,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Macro F1-Score: {f1:.2f}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=rf_model.classes_)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=rf_model.classes_, yticklabels=rf_model.classes_)\n",
    "plt.title(\"Confusion Matrix for Random Forest Model\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# 13️⃣ 특정 PC(E013) 로그 분석\n",
    "# ============================================\n",
    "from collections import Counter\n",
    "logs_e013 = pc_groups.get_group('E013')['merge'].values\n",
    "e013_counter = Counter([item for log in logs_e013 for item in log])\n",
    "top_5_e013 = e013_counter.most_common(5)\n",
    "\n",
    "print(\"\\nTop 5 frequent logs for E013:\")\n",
    "for item, count in top_5_e013:\n",
    "    print(f\"{item}: {count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
