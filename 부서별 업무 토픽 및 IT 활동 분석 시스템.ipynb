{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2df08f-d388-4760-b7ba-553e0edd5f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1ï¸âƒ£ ë¶€ì„œë³„ ì„œë¸Œ ë¶€ì„œ í™•ì¸\n",
    "# ============================================\n",
    "sub_departments_by_dept = jd.groupby('department')['sub_department'].unique()\n",
    "print(sub_departments_by_dept)\n",
    "# ì‹¤í–‰ ê²°ê³¼ ìš”ì•½: ì „ì²´ ë¶€ì„œ ê°œìˆ˜ì™€ ê° ë¶€ì„œë³„ ì„œë¸Œ ë¶€ì„œ ë¦¬ìŠ¤íŠ¸ê°€ ì¶œë ¥ë©ë‹ˆë‹¤.\n",
    "\n",
    "# ============================================\n",
    "# 2ï¸âƒ£ íŠ¹ì • ì§ë¬´ ì†Œê°œ ë¬¸êµ¬ ì •ì œ\n",
    "# ============================================\n",
    "import re\n",
    "\n",
    "# ì§ë¬´ ì„ íƒ\n",
    "original_text = jd[jd['job_title'] == 'ìŠ¹ìš© ì¡°ë¦½ì°¨ ìƒì‚°ì§€ì›']['organization_intro'].iloc[0]\n",
    "print(\"ì •ì œ ì „ í…ìŠ¤íŠ¸:\\n\", original_text)\n",
    "\n",
    "# í•œê¸€ê³¼ ê³µë°±ë§Œ ë‚¨ê¸°ê¸°\n",
    "def clean_text(text):\n",
    "    return re.sub('[^ê°€-í£\\s]', '', text)\n",
    "\n",
    "cleaned_text = clean_text(original_text)\n",
    "print(\"\\nì •ì œ í›„ í…ìŠ¤íŠ¸:\\n\", cleaned_text)\n",
    "\n",
    "# ============================================\n",
    "# 3ï¸âƒ£ í•œêµ­ì–´ NLP íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ\n",
    "# ============================================\n",
    "!pip install -q konlpy nltk spacy\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "!python -m spacy download ko_core_news_sm\n",
    "\n",
    "# ============================================\n",
    "# 4ï¸âƒ£ í† í°í™” ë° ë¹„êµ\n",
    "# ============================================\n",
    "import time\n",
    "from konlpy.tag import Okt, Kkma\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"ko_core_news_sm\")\n",
    "okt = Okt()\n",
    "kkma = Kkma()\n",
    "\n",
    "# 1. Okt í† í°í™”\n",
    "start_time = time.time()\n",
    "okt_tokens = okt.morphs(cleaned_text)\n",
    "okt_time = time.time() - start_time\n",
    "\n",
    "# 2. Kkma í† í°í™”\n",
    "start_time = time.time()\n",
    "kkma_tokens = kkma.morphs(cleaned_text)\n",
    "kkma_time = time.time() - start_time\n",
    "\n",
    "# 3. NLTK word_tokenize\n",
    "start_time = time.time()\n",
    "nltk_tokens = nltk.word_tokenize(cleaned_text)\n",
    "nltk_time = time.time() - start_time\n",
    "\n",
    "# 4. spaCy í† í°í™”\n",
    "start_time = time.time()\n",
    "spacy_tokens = [token.text for token in nlp(cleaned_text)]\n",
    "spacy_time = time.time() - start_time\n",
    "\n",
    "# ì¶œë ¥\n",
    "print(f\"okt: {okt_tokens} ({okt_time:.4f}ì´ˆ)\")\n",
    "print(f\"kkma: {kkma_tokens} ({kkma_time:.4f}ì´ˆ)\")\n",
    "print(f\"word_tokenize: {nltk_tokens} ({nltk_time:.4f}ì´ˆ)\")\n",
    "print(f\"spaCy: {spacy_tokens} ({spacy_time:.4f}ì´ˆ)\")\n",
    "\n",
    "# ============================================\n",
    "# 5ï¸âƒ£ í’ˆì‚¬ íƒœê¹… ë° ëª…ì‚¬ ì¶”ì¶œ\n",
    "# ============================================\n",
    "pos_tags = okt.pos(cleaned_text)\n",
    "morphs = okt.morphs(cleaned_text)\n",
    "nouns = [word for word, pos in pos_tags if pos == 'Noun']\n",
    "print(\"í’ˆì‚¬ íƒœê¹…:\", pos_tags)\n",
    "print(\"ì–´ê°„ ì¶”ì¶œ:\", morphs)\n",
    "print(\"ëª…ì‚¬ ì¶”ì¶œ:\", nouns)\n",
    "\n",
    "# ============================================\n",
    "# 6ï¸âƒ£ ë¶ˆìš©ì–´ ì œê±°\n",
    "# ============================================\n",
    "stopwords = {\n",
    "    'ìš°ë¦¬', 'ìê¸°', 'ê·¸', 'ê·¸ê²ƒ', 'ì´', 'ì´ê²ƒ', 'ì €', 'ë‚˜', 'ë„ˆ', 'ë“±', 'ëŒ€í•´', 'ë°', \n",
    "    'ê·¸ëŸ°', 'ë˜', 'í•˜ì§€ë§Œ', 'ì™œ', 'ë•Œë¬¸', 'ìœ¼ë¡œ', 'ê°€', 'ì˜', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ì™€', \n",
    "    'ê³¼', 'ì„', 'ë¥¼', 'ìœ¼ë¡œ', 'ëŠ”', 'ì€', 'ì…ë‹ˆë‹¤', 'ë“¤', 'ë§Œ', 'ê²ƒ', 'ê°™ë‹¤', 'ì¤‘',\n",
    "    'ì‚¬ëŒ', 'í›„', 'ì°¨', 'ì „', 'ë³„', 'ì ê²€', 'ë¬¼ëŸ‰', 'í™˜ê²½', 'íƒ„ë ¥', 'ë¬¼ë¥˜', 'ì¡°ì •'\n",
    "}\n",
    "filtered_nouns = [word for word in nouns if word not in stopwords]\n",
    "print(\"ë¶ˆìš©ì–´ ì œê±° í›„ ëª…ì‚¬:\", filtered_nouns)\n",
    "\n",
    "# 'tokens' ì»¬ëŸ¼ ì¶”ê°€\n",
    "psatcar_jd.loc[psatcar_jd['job_title']=='ìŠ¹ìš© ì¡°ë¦½ì°¨ ìƒì‚°ì§€ì›','tokens'] = [' '.join(filtered_nouns)]\n",
    "print(psatcar_jd[psatcar_jd['job_title']=='ìŠ¹ìš© ì¡°ë¦½ì°¨ ìƒì‚°ì§€ì›'][['organization_intro','tokens']].head())\n",
    "\n",
    "# ============================================\n",
    "# 7ï¸âƒ£ LDA í† í”½ ëª¨ë¸ë§\n",
    "# ============================================\n",
    "!pip install gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "# organization_intro ì •ì œ í›„ ëª…ì‚¬ ì¶”ì¶œ\n",
    "jd['clean_text'] = jd['organization_intro'].str.replace(r'[^ê°€-í£\\s]', '', regex=True)\n",
    "jd['nouns'] = jd['clean_text'].apply(lambda doc: okt.nouns(doc) if isinstance(doc,str) else [])\n",
    "\n",
    "# ì „ì²´ ëª…ì‚¬ ë¹ˆë„ ê³„ì‚°\n",
    "from collections import Counter\n",
    "all_nouns = [n for nouns in jd['nouns'] for n in nouns]\n",
    "noun_freq = Counter(all_nouns)\n",
    "freq_df = pd.DataFrame(noun_freq.items(), columns=['noun','freq']).sort_values('freq',ascending=False).reset_index(drop=True)\n",
    "print(freq_df.head(20))\n",
    "\n",
    "# ìƒìœ„ 10ê°œ ë‹¨ì–´ë¥¼ ë¶ˆìš©ì–´ ì²˜ë¦¬\n",
    "top10_stopwords = set(freq_df.head(10)['noun'])\n",
    "jd['tokens'] = jd['nouns'].apply(lambda noun_list: [w for w in noun_list if w not in top10_stopwords])\n",
    "empty_tokens = jd['tokens'].map(len)==0\n",
    "print(f\"tokens ë¹ˆ ë¦¬ìŠ¤íŠ¸ì¸ í–‰ ê°œìˆ˜: {empty_tokens.sum()}\")\n",
    "print(jd[['nouns','tokens']].head())\n",
    "\n",
    "# LDA ëª¨ë¸ í•™ìŠµ\n",
    "texts = jd['tokens'].tolist()\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in texts]\n",
    "\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=6, random_state=42, update_every=1, chunksize=100, passes=20, alpha='auto', per_word_topics=True)\n",
    "\n",
    "for tid, terms in lda_model.show_topics(num_topics=6, formatted=False):\n",
    "    print(f\"Topic {tid}: {[w for w,p in terms]}\")\n",
    "\n",
    "# ============================================\n",
    "# 8ï¸âƒ£ ë¶€ì„œë³„ LDA ëª¨ë¸ë§\n",
    "# ============================================\n",
    "grouped = jd.groupby('department')['tokens'].apply(list).to_dict()\n",
    "lda_models = {}\n",
    "for sub_dept, tokens_list in grouped.items():\n",
    "    texts = tokens_list\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    num_topics = len(tokens_list)\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=15, random_state=42)\n",
    "    lda_models[sub_dept] = lda_model\n",
    "\n",
    "    print(f\"ë¶€ì„œ: {sub_dept}\")\n",
    "    for idx, topic in lda_model.print_topics(num_words=5, num_topics=5):\n",
    "        print(f\"ğŸŸ¦ ì£¼ì œ {idx+1}: {topic}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# pyLDAvis ì‹œê°í™”\n",
    "pyLDAvis.enable_notebook()\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from joblib import parallel_backend\n",
    "with parallel_backend('threading', n_jobs=1):\n",
    "    vis_data = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "vis_data\n",
    "\n",
    "# ============================================\n",
    "# 9ï¸âƒ£ IT ë¡œê·¸ ë°ì´í„° ì²˜ë¦¬\n",
    "# ============================================\n",
    "import json\n",
    "with open('IT_logs.json','r',encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "log = pd.DataFrame(data)\n",
    "log['timestamp'] = pd.to_datetime(log['timestamp'])\n",
    "log['hour'] = log['timestamp'].dt.hour\n",
    "log['PC'].value_counts()\n",
    "log['program'] = log['window_title'].str.extract(r'\\[(.*?)\\]')\n",
    "log['file'] = log['window_title'].str.extract(r'-\\s(.+)$')\n",
    "log['merge'] = log['window_title'].str.extract(r'\\[.*\\]\\s*(.*)\\s*-\\s*(.*)')[1]\n",
    "log['day'] = log['timestamp'].dt.date\n",
    "log_grouped = log.groupby(['PC','day'])['merge'].apply(lambda x: list(x)).reset_index()\n",
    "print(log_grouped.head())\n",
    "\n",
    "# ============================================\n",
    "# 10ï¸âƒ£ Word2Vec ì„ë² ë”©\n",
    "# ============================================\n",
    "from gensim.models import Word2Vec\n",
    "sentences = log_grouped['merge'].tolist()\n",
    "model = Word2Vec(min_count=1, window=5, vector_size=30, workers=4)\n",
    "model.build_vocab(sentences, progress_per=10000)\n",
    "print(\"Vocabulary Size:\", len(model.wv))\n",
    "print(\"Example words:\", list(model.wv.index_to_key)[:10])\n",
    "\n",
    "# Skip-Gram ëª¨ë¸\n",
    "model = Word2Vec(sentences=sentences, vector_size=30, window=5, min_count=1, sg=1, workers=4, epochs=10)\n",
    "word_vectors = model.wv\n",
    "print(\"Words in vocabulary:\", list(word_vectors.index_to_key)[:10])\n",
    "\n",
    "# ============================================\n",
    "# 11ï¸âƒ£ t-SNE ì‹œê°í™”\n",
    "# ============================================\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pc_groups = log_grouped.groupby('PC')\n",
    "pc_vectors = []\n",
    "labels = []\n",
    "\n",
    "for pc, group in pc_groups:\n",
    "    for log in group['merge']:\n",
    "        log_vector = np.mean([model.wv[word] for word in log if word in model.wv], axis=0)\n",
    "        if log_vector is not None:\n",
    "            pc_vectors.append(log_vector)\n",
    "            labels.append(pc)\n",
    "\n",
    "pc_vectors_flattened = np.array(pc_vectors)\n",
    "perplexity_value = min(30, len(pc_vectors_flattened)-1)\n",
    "tsne = TSNE(n_components=2, perplexity=perplexity_value, learning_rate=200)\n",
    "transformed_vectors = tsne.fit_transform(pc_vectors_flattened)\n",
    "label_mapping = {label: idx for idx,label in enumerate(sorted(set(labels)))}\n",
    "numeric_labels = [label_mapping[label] for label in labels]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "scatter = plt.scatter(transformed_vectors[:,0], transformed_vectors[:,1], c=numeric_labels, cmap='tab20')\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=label_mapping.keys())\n",
    "plt.title(\"100-day Logs Distribution by PC\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.colorbar(scatter)\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# 12ï¸âƒ£ RandomForest ê¸°ë°˜ PC ë¶„ë¥˜\n",
    "# ============================================\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(pc_vectors_flattened, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=9,\n",
    "    max_depth=3,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Macro F1-Score: {f1:.2f}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=rf_model.classes_)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=rf_model.classes_, yticklabels=rf_model.classes_)\n",
    "plt.title(\"Confusion Matrix for Random Forest Model\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# 13ï¸âƒ£ íŠ¹ì • PC(E013) ë¡œê·¸ ë¶„ì„\n",
    "# ============================================\n",
    "from collections import Counter\n",
    "logs_e013 = pc_groups.get_group('E013')['merge'].values\n",
    "e013_counter = Counter([item for log in logs_e013 for item in log])\n",
    "top_5_e013 = e013_counter.most_common(5)\n",
    "\n",
    "print(\"\\nTop 5 frequent logs for E013:\")\n",
    "for item, count in top_5_e013:\n",
    "    print(f\"{item}: {count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
